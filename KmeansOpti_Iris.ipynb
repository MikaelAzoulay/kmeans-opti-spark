{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KmeansOpti_Iris.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfLsQCp9mYJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        " \n",
        " \n",
        "from pyspark import SparkContext, SparkConf\n",
        "from math import sqrt\n",
        "from numpy.linalg import norm \n",
        "import numpy as np\n",
        "import time\n",
        "import sys\n",
        " \n",
        " \n",
        "def computeDistance(x,y):\n",
        "    return sqrt(sum([(a - b)**2 for a,b in zip(x,y)]))\n",
        " \n",
        " \n",
        "def closestCluster(dist_list):\n",
        "    cluster = dist_list[0][0]\n",
        "    min_dist = dist_list[0][1]\n",
        "    for elem in dist_list:\n",
        "        if elem[1] < min_dist:\n",
        "            cluster = elem[0]\n",
        "            min_dist = elem[1]\n",
        "    return (cluster,min_dist)\n",
        " \n",
        "def sumList(x,y):\n",
        "    return [x[i]+y[i] for i in range(len(x))]\n",
        " \n",
        "def moyenneList(x,n):\n",
        "    return [x[i]/n for i in range(len(x))]\n",
        " \n",
        " \n",
        "def choice(p):\n",
        "    \n",
        "    #Generates a random sample from [0, len(p)),\n",
        "    #where p[i] is the probability associated with i. \n",
        "    \n",
        "    random = np.random.random()\n",
        "    r = 0.0\n",
        "    for idx in range(len(p)):\n",
        "        r = r + p[idx]\n",
        "        if r > random:\n",
        "            return idx\n",
        "    assert(False)\n",
        " \n",
        " \n",
        "def KmeansOpti_init(rdd, K, RUNS):\n",
        "    \n",
        "    ########################################################\n",
        "    # Select `RUNS` sets of initial points for `K`-means++ #\n",
        "    ########################################################\n",
        "    \n",
        "    # the `centroides` variable is what we want to return\n",
        "    n_data = rdd.count()\n",
        "    #shape = rdd.take(1)[0][1].shape[0]\n",
        "    centroides = np.zeros((RUNS, K, 4))\n",
        "    \n",
        "    def update_dist(vec, dist, k):\n",
        "        new_dist = norm(vec - centroides[:, k], axis=1)**2\n",
        "        return np.min([dist, new_dist], axis=0)\n",
        "    \n",
        "    # The second element `dist` in the tuple below is the\n",
        "    # closest distance from each data point to the selected\n",
        "    # points in the initial set\n",
        "    data = (rdd.map(lambda p: (p, [np.inf] * RUNS)).cache())\n",
        "    # Collect the feature vectors of all data points\n",
        "    local_data = (rdd.map(lambda x: x[1]).collect())\n",
        "    # Randomly select the first point for every run ofk-means++\n",
        "    sample = [local_data[k] for k in\n",
        "        np.random.randint(0, len(local_data), RUNS)]\n",
        "    centroides[:, 0] = sample\n",
        "\n",
        "    for idx in range(K - 1):\n",
        "        #Update distance\n",
        "        data = (data.map(lambda x: ((x[0][0],x[0][1]),update_dist(x[0][1],x[1],idx))).cache())\n",
        "        #Calculate sum of D_i(x)^2\n",
        "        d1 = data.map(lambda x: (1,x[1]))\n",
        "        d2 = d1.reduceByKey(lambda x,y: np.sum([x,y], axis=0))\n",
        "        total = d2.collect()[0][1]\n",
        "        #Normalize each distance to get the probabilities and reshape\n",
        "        prob = (data.map(lambda x: np.divide(x[1],total)).collect())\n",
        "        prob = np.reshape(prob,(len(local_data), RUNS))\n",
        "        #K'th centroid for each run\n",
        "        data_id = [choice(prob[:,i]) for i in range(RUNS)]\n",
        "        sample = [local_data[i] for i in data_id]\n",
        "        centroides[:, idx+1] = sample\n",
        "    return centroides\n",
        " \n",
        " \n",
        "def KmeansOpti(data, nb_clusters, max_steps, RUNS, min_switch = 0, init=\"kmeans++\"):\n",
        "    clusteringDone = False\n",
        "    number_of_steps = 0\n",
        "    current_error = float(\"inf\")\n",
        "    # A broadcast value is sent to and saved  by each executor for further use\n",
        "    # instead of being sent to each executor when needed.\n",
        "    nb_elem = sc.broadcast(data.count())\n",
        " \n",
        "    ##################################################\n",
        "    # Select initial centroides with Kmeans++ method #\n",
        "    ##################################################\n",
        "\n",
        "    data_no_label = data.map(lambda x : (x[0],x[1][:4]))\n",
        "\n",
        "    if init==\"kmeans++\":\n",
        "        init=KmeansOpti_init(data_no_label,nb_clusters,RUNS)\n",
        "        #create a unique key for each cluster of each run \n",
        "        #key = (Run_id,Cluster_id)\n",
        "        centroides = sc.parallelize(init.reshape(RUNS*nb_clusters,4)).map(lambda x:[float(i) for i in x])\\\n",
        "              .zipWithIndex()\\\n",
        "              .map(lambda x: ((x[1]//nb_clusters,x[1]%nb_clusters),x[0]))\n",
        "        print(\"Kmeans++ initialization done.\")\n",
        "        print(\"init centroides = \",centroides.collect())\n",
        "    else:\n",
        "        centroides = sc.parallelize(data_no_label.takeSample('withoutReplacment',nb_clusters*RUNS))\\\n",
        "              .zipWithIndex()\\\n",
        "              .map(lambda x: ((x[1]//nb_clusters,x[1]%nb_clusters),x[0][1][:-1]))\n",
        "        print(\"Simple Kmeans initialization done.\")\n",
        "        print(\"init centroides = \",centroides.collect())\n",
        "\n",
        "    #centroides example :\n",
        "    #((0, 0), [4.9, 3.0, 1.4, 0.2])\n",
        "    #((RUN, center), [4.9, 3.0, 1.4, 0.2])\n",
        "\n",
        "    while not clusteringDone:\n",
        "        \n",
        "        start=time.time()\n",
        "        print(\"step num = \",number_of_steps)\n",
        " \n",
        "        #############################\n",
        "        # Assign points to clusters #\n",
        "        #############################\n",
        " \n",
        "        joined = data.cartesian(centroides)\n",
        "        # ((0, [5.1, 3.5, 1.4, 0.2, 'Iris-setosa']), ((0, 0), [4.4, 3.0, 1.3, 0.2]))\n",
        "        # ((point, [5.1, 3.5, 1.4, 0.2, 'Iris-setosa']), ((RUN, center), [4.4, 3.0, 1.3, 0.2]))\n",
        " \n",
        "        # We compute the distance between the points and each cluster\n",
        "        dist = joined.map(lambda x: ((x[0][0],x[1][0][0]),(x[1][0][1], computeDistance(x[0][1][:-1], x[1][1]))))\n",
        "        # ((0, 0), (0, 0.866025403784438))\n",
        "        # ((point, RUN), (center, dist))\n",
        " \n",
        "        dist_list = dist.groupByKey().mapValues(list)\n",
        "        \n",
        "        # ((0, 0), [(0, 0.866025403784438), (1, 3.7), (2, 0.5385164807134504)])\n",
        "        # ((point, RUN), [(center1, dist1), (center2, dist2), (center3, dist3)])\n",
        " \n",
        "        # We keep only the closest cluster to each point.\n",
        "        min_dist = dist_list.mapValues(closestCluster).map(lambda x:((x[0][0], x[0][1]), (x[1][0], x[1][1])))\n",
        "        # ((0, 0), (2, 0.5385164807134504))\n",
        " \n",
        "        # assignment will be our return value : It contains the datapoint,\n",
        "        # the id of the closest cluster and the distance of the point to the centroid\n",
        "        assignment = min_dist.map(lambda x:(x[0][0], (x[0][1], x[1][0], x[1][1]))).join(data)\n",
        " \n",
        "        # (0, ((0, 2, 0.5385164807134504), [5.1, 3.5, 1.4, 0.2, 'Iris-setosa']))\n",
        "        # (point, ((RUN, center, dist), [5.1, 3.5, 1.4, 0.2, 'Iris-setosa']))\n",
        " \n",
        "        ############################################\n",
        "        # Compute the new centroid of each cluster #\n",
        "        ############################################\n",
        " \n",
        "        clusters = assignment.map(lambda x: ((x[1][0][0],x[1][0][1]), x[1][1][:-1]))\n",
        "        # ((0, 2), [5.1, 3.5, 1.4, 0.2])\n",
        "        # ((RUN, center), [5.1, 3.5, 1.4, 0.2])\n",
        "        \n",
        "        count = clusters.map(lambda x: (x[0],1)).reduceByKey(lambda x,y: x+y)\n",
        "        somme = clusters.reduceByKey(sumList)\n",
        "        centroidesCluster = somme.join(count).map(lambda x : (x[0],moyenneList(x[1][0],x[1][1])))\n",
        " \n",
        "        ############################\n",
        "        # Is the clustering over ? #\n",
        "        ############################\n",
        " \n",
        "        # Let's see how many points have switched clusters.\n",
        "        if number_of_steps > 0:\n",
        "            switchTAB=prev_assignment.join(min_dist).map(lambda x: (x[0][1],1) if (x[1][0][0] != x[1][1][0]) else (x[0][1],0)).reduceByKey(lambda x,y: x+y).map(lambda x: x[1])\n",
        "            print(switchTAB.collect())\n",
        "            switch=switchTAB.reduce(lambda x,y: x+y)\n",
        "            print(\"total_switch = \",switch)\n",
        "        else:\n",
        "            switch = nb_elem.value*RUNS\n",
        "        if switch <= min_switch or number_of_steps == max_steps:\n",
        "            clusteringDone = True\n",
        "            sum_dist = min_dist.map(lambda x: (x[0][1],x[1][1])).reduceByKey(lambda x,y: x + y)\n",
        "            avg_dist = sum_dist.map(lambda x : x[1]).reduce(lambda x,y:x+y)/RUNS\n",
        "            print(\"avg_dist = \",avg_dist)\n",
        "            error=sum_dist.map(lambda x : sqrt(x[1])/nb_elem.value).collect()\n",
        "            best_error=min(error)\n",
        "            best_RUN=error.index(best_error)\n",
        "            best_assignment=assignment.filter(lambda x:x[1][0][0]==best_RUN).map(lambda x:(x[0], ((x[1][0][1],x[1][0][2]), x[1][1])))\n",
        "            avg_error=sum(error)/RUNS\n",
        "            print(avg_error)\n",
        "            worst_error=max(error)\n",
        "            print(worst_error)\n",
        "        else:\n",
        "            centroides = centroidesCluster\n",
        "            prev_assignment = min_dist\n",
        "            number_of_steps += 1\n",
        "        print(\"time = \",time.time() - start)\n",
        "        print(\"avg_switch = \",switch)\n",
        " \n",
        "    return (best_assignment, best_error, number_of_steps)\n",
        " \n",
        " \n",
        " \n",
        "if __name__ == \"__main__\":\n",
        " \n",
        "    conf = SparkConf().setAppName('exercice')\n",
        "    sc = SparkContext(conf=conf)\n",
        "\n",
        "    if len(sys.argv) != 6:\n",
        "      print(\"You have to enter 5 arguments\")\n",
        "      print(\" 1 Path to the data text file\")\n",
        "      print(\" 2 Path to the output file\")\n",
        "      print(\" 3 Number K of clusters\")\n",
        "      print(\" 4 Number S of Steps\")\n",
        "      print(\" 5 Number R of RUNS\")\n",
        "      print(\"for example : spark-submit KmeansOpti.py path/to/the/file/iris.data.txt 3 5 2\")\n",
        "      exit(0)\n",
        "\n",
        "    # inputs\n",
        "    input_path = sys.argv[1]  # path to the data file\n",
        "    output_path = sys.argv[2]  # path to the output folder\n",
        "    nb_clusters = int(sys.argv[3]) # number of clusters\n",
        "    max_steps = int(sys.argv[4]) # maximum number of iterations\n",
        "    RUNS = int(sys.argv[5]) # number of RUNS\n",
        "    #min_switch = int(sys.argv[6]) # minimum number of switches needed to converge, default = 0\n",
        "    #init = int(sys.argv[7]) # wich kind of initialization, default = \"kmeans++\"\"\n",
        " \n",
        "    lines = sc.textFile(input_path)\n",
        "    data = lines.map(lambda x: x.split(','))\\\n",
        "            .map(lambda x: [float(i) for i in x[:4]]+[x[4]])\\\n",
        "            .zipWithIndex()\\\n",
        "            .map(lambda x: (x[1],x[0]))\n",
        "    # zipWithIndex allows us to give a specific index to each point\n",
        "    # (0, [5.1, 3.5, 1.4, 0.2, 'Iris-setosa'])\n",
        "\n",
        "    st = time.time()\n",
        "    clusteringOpti = KmeansOpti(data,nb_clusters,max_steps,RUNS)\n",
        "\n",
        "    execution_time = time.time() - st\n",
        "    #unique identifier for our result file\n",
        "    identifier = str(int(time.time())-1574248997)\n",
        "    info = sc.parallelize([\"id = \" + identifier\\\n",
        "\t    , \"name = KmeansOpti.py\"\\\n",
        "            , \"RUNS = \" + str(RUNS)\\\n",
        "            , \"K clusters = \" + str(nb_clusters)\\\n",
        "            , \"max steps = \" + str(max_steps)\\\n",
        "            ,\"#######RESULT######################\\n\"\\\n",
        "            , \"time = \" + str(execution_time)\\\n",
        "            , \"error = \" + str(clusteringOpti[1])\\\n",
        "            , \"steps = \" + str(clusteringOpti[2])])\n",
        "\n",
        "    print (\"Time takes to converge with KmeansOpti:\", execution_time )\n",
        "    print(\"errorOpti=\",clusteringOpti[1],\"stepsOpti=\",clusteringOpti[2])\n",
        "    print(\"assignmentTOTAL=\",clusteringOpti[0].count(),\"assignment example\",clusteringOpti[0].take(1))\n",
        "\n",
        "    #save the result\n",
        "\n",
        "    result_file = output_path+\"/KmeansOpti\"+identifier\n",
        "    info_file = output_path+\"/KmeansOpti\"+identifier+\"info\"\n",
        "    clusteringOpti[0].saveAsTextFile(result_file)\n",
        "    # if you want to have only 1 file as a result\n",
        "    result_file_unique = output_path+\"/1KmeansOpti\"+identifier\n",
        "    clusteringOpti[0].coalesce(1).saveAsTextFile(result_file_unique)\n",
        "\n",
        "    info.coalesce(1).saveAsTextFile(info_file)\n",
        " \n",
        "    print(clusteringOpti)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}